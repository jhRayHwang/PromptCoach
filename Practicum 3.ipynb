{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166293bd-b099-41df-ad1d-e709a737fa6f",
   "metadata": {},
   "source": [
    "# What impact does emotional tone (anger, sadness, and anxiety) in prompts have on the Large Language Model responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fba9f1-428f-4ef3-a8db-ae4540e575d8",
   "metadata": {},
   "source": [
    "Which emotion has the highest impact on quality in LLM responses? We believe that such emotions (anger, sadness, and anxiety) may have varying effects on LLM output quality. For example, prompts with high anxiety may produce 'better' prompts because the model could sense the urgency in the situation.\n",
    "\n",
    "How can we do this? We must curate public datasets of real conversations between users and LLMs. In this project we will primarily be working with WildChat and ShareGPT52k, which hold around 1 million conversations.\n",
    "\n",
    "To analyze emotional tone in prompts, we utilize a proprietary service, LIWC, that calculates the percentage of words in a sentence that relates to specific categories. To measure Large Language Model responses, we will either use BigBench or HELM, whichever we get to work. Additionally, we use matplotlib and seaborn to visualize our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e864dc-0783-4ac1-b2f0-a9f61d446f03",
   "metadata": {},
   "source": [
    "In step 1 we load the data from HuggingFace and filter them based on keywords we believe pertain to the outputs we're measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "273da096-cf3d-41c0-8c3c-42ea46ab000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "from datasets import load_dataset\n",
    "wildchat = load_dataset(\"allenai/WildChat-1M\", split='train')\n",
    "sharegpt = load_dataset('RyokoAI/ShareGPT52K', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98707d40-b756-4b14-8b89-2ed424ee5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildchat_convo = []\n",
    "\n",
    "MAX_WORD_COUNT = 50  # Set a reasonable word count limit for prompts\n",
    "\n",
    "for conversation in wildchat['conversation']:\n",
    "    # Collect only the first turn of each conversation\n",
    "    user_turn = conversation[0]\n",
    "\n",
    "    if user_turn.get('language') == \"English\":\n",
    "        prompt = user_turn.get('content', '').strip().lower()\n",
    "\n",
    "        # Ensure the prompt is not too large\n",
    "        if len(prompt.split()) < MAX_WORD_COUNT:\n",
    "            wildchat_convo.append({\n",
    "                'prompt': user_turn['content'],\n",
    "                'response': conversation[1].get('content', '').strip() if len(conversation) > 1 else ''\n",
    "            })\n",
    "            \n",
    "    if len(wildchat_convo) >= 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0468dc39-9e93-4d71-9da5-d9b8869e0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "sharegpt_convo = []\n",
    "\n",
    "# Helper function\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "MAX_WORD_COUNT = 50  # Set a reasonable word count limit\n",
    "\n",
    "for example in sharegpt:\n",
    "    messages = example.get(\"conversations\", [])\n",
    "\n",
    "    if not isinstance(messages, list) or not all(isinstance(m, dict) for m in messages):\n",
    "        continue\n",
    "\n",
    "    if len(messages) < 2:\n",
    "        continue  # Need at least one user and one assistant message\n",
    "\n",
    "    user = messages[0]\n",
    "    bot = messages[1]\n",
    "\n",
    "    prompt = user.get(\"value\", \"\").strip()\n",
    "    response = bot.get(\"value\", \"\").strip()\n",
    "\n",
    "    if (\n",
    "        is_english(prompt)\n",
    "        and is_english(response)\n",
    "        and len(prompt.split()) < MAX_WORD_COUNT\n",
    "    ):\n",
    "        sharegpt_convo.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "    if len(sharegpt_convo) >= 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1c31f3-4b1e-4733-93ad-ac5bbd2085b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "convos = wildchat_convo + sharegpt_convo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d86417-e42d-4b45-8c02-455583e6224b",
   "metadata": {},
   "source": [
    "In step 2, we will run our natural prompts through LangChain using the gemma2-9b-it and llama-3.3-70b-versatile supported models to variate our prompts and produce corresponding model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79073e93-323d-48c8-b5e5-1f513b884670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-groq  langchain-core\n",
    "\n",
    "GROQ_API_KEY=\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=2, groq_api_key=GROQ_API_KEY, model_name=\"gemma2-9b-it\") # And 'llama-3.3-70b-versatile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f38e09-7521-447d-af2c-fab612954080",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are an assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "chain = prompt | chat\n",
    "\n",
    "def emo_gen(text, emotion):\n",
    "    response = chain.invoke({\n",
    "        \"text\": f\"\"\"Generate 5 rephrasings of the following prompt that reflect increasing levels of {emotion}, from 1 (very mild) to 5 (extremely intense). \n",
    "Number each version on a new line like this (keep your answer between 150-200 words with no emojis):\n",
    "\n",
    "1. [mild {emotion}]\n",
    "2. [slightly more intense {emotion}]\n",
    "3. ...\n",
    "5. [most intense {emotion}]\n",
    "\n",
    "Prompt: \"{text}\"\n",
    "\"\"\"\n",
    "    })\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9c9475d-82ef-47b4-9ead-95ae81a712b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"(?m)^(\\d+)\\.\\s+(.*)\"\n",
    "\n",
    "anger_prompts = []\n",
    "sad_prompts = []\n",
    "anxious_prompts = []\n",
    "\n",
    "joyful_prompts = []\n",
    "gratitude_prompts = []\n",
    "hopeful_prompts = []\n",
    "\n",
    "for row in convos:\n",
    "    prompt = row['prompt']\n",
    "\n",
    "    anger_prompt = emo_gen(prompt, \"anger\")\n",
    "    sad_prompt = emo_gen(prompt, \"sadness\")\n",
    "    anxious_prompt = emo_gen(prompt, \"anxious\")\n",
    "\n",
    "    joyful_prompt = emo_gen(prompt, \"Joyfulness\")\n",
    "    gratitude_prompt = emo_gen(prompt, \"Gratitude\")\n",
    "    hopeful_prompt = emo_gen(prompt, \"Hopefulness\")\n",
    "    \n",
    "    anger_sentences = [s for _, s in re.findall(pattern, anger_prompt)]\n",
    "    sad_sentences = [s for _, s in re.findall(pattern, sad_prompt)]\n",
    "    anxious_sentences = [s for _, s in re.findall(pattern, anxious_prompt)]\n",
    "\n",
    "    joyful_sentences = [s for _, s in re.findall(pattern, joyful_prompt)]\n",
    "    gratitude_sentences = [s for _, s in re.findall(pattern, gratitude_prompt)]\n",
    "    hopeful_sentences = [s for _, s in re.findall(pattern, hopeful_prompt)]\n",
    "    \n",
    "    if len(anger_sentences) == 5:\n",
    "        anger_prompts.append(anger_sentences)\n",
    "    if len(sad_sentences) == 5:\n",
    "        sad_prompts.append(sad_sentences)\n",
    "    if len(anxious_sentences) == 5:\n",
    "        anxious_prompts.append(anxious_sentences)\n",
    "\n",
    "    if len(joyful_sentences) == 5:\n",
    "        joyful_prompts.append(joyful_sentences)\n",
    "    if len(gratitude_sentences) == 5:\n",
    "        gratitude_prompts.append(gratitude_sentences)\n",
    "    if len(hopeful_sentences) == 5:\n",
    "        hopeful_prompts.append(hopeful_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94165c-bcab-4799-91e9-3eb837a73eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_emotion_responses(prompt_groups, label=\"Emotion\"):\n",
    "    output = []\n",
    "\n",
    "    for prompt_group in tqdm(prompt_groups, desc=f\"Generating responses for {label}\"):\n",
    "        for i, prompt_text in enumerate(prompt_group):\n",
    "            intensity = i + 1  # Levels 1–5\n",
    "\n",
    "            try:\n",
    "                response = chain.invoke({\n",
    "                    \"text\": f\"{prompt_text}\\n\\n(Keep your answer between 300-400 words with no emojis.)\"\n",
    "                })\n",
    "\n",
    "                output.append({\n",
    "                    \"intensity\": intensity,\n",
    "                    \"prompt\": prompt_text,\n",
    "                    \"response\": response.content\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{label}] Error with prompt: {prompt_text}\\n{e}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "anger_convos = generate_emotion_responses(anger_prompts, label=\"Anger\")\n",
    "sad_convos = generate_emotion_responses(sad_prompts, label=\"Sad\")\n",
    "anxious_convos = generate_emotion_responses(anxious_prompts, label=\"Anxious\")\n",
    "\n",
    "joyful_convos = generate_emotion_responses(joyful_prompts, label=\"Joyfulness\")\n",
    "gratitude_convos = generate_emotion_responses(gratitude_prompts, label=\"Gratitude\")\n",
    "hopeful_convos = generate_emotion_responses(hopeful_prompts, label=\"Hopefulness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06d978d6-4450-43d9-9f65-20364dc07e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "convos = {\n",
    "    \"anger\": anger_convos,\n",
    "    \"sad\": sad_convos,\n",
    "    \"anxious\": anxious_convos,\n",
    "    \"joyful\": joyful_convos,\n",
    "    \"gratitude\": gratitude_convos,\n",
    "    \"hopeful\": hopeful_convos\n",
    "}\n",
    "\n",
    "with open(\"gemma_convos.json\", \"w\") as f:\n",
    "    json.dump(convos, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c817f0-968e-4bc0-ac40-cc57a1426696",
   "metadata": {},
   "source": [
    "In step 3, we measure our outputs using metrics of perplexity, lexical diversity, and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612f3d5-a8e2-4deb-b791-c59a25bd7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# --- Perplexity Calculator ---\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(\n",
    "            torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "    def perplexity(self, text):\n",
    "        enc = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=1024).to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            loss = self.model(**enc, labels=enc['input_ids']).loss\n",
    "        return torch.exp(loss).item()\n",
    "\n",
    "# --- Lexical diversity ---\n",
    "def lexical_diversity(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "\n",
    "# --- Evaluate a list of responses ---\n",
    "def evaluate_data(data, pp_calc):\n",
    "    records = []\n",
    "    for item in data:\n",
    "        resp = item.get('response', '')\n",
    "        records.append({\n",
    "            'intensity': item.get('intensity', None),\n",
    "            'readability': textstat.flesch_reading_ease(resp),\n",
    "            'perplexity': pp_calc.perplexity(resp),\n",
    "            'lex_diversity': lexical_diversity(resp)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def plot_grouped_metric(all_metrics, metric_name):\n",
    "    for model in all_metrics:\n",
    "        combined = []\n",
    "        for emotion, df in all_metrics[model].items():\n",
    "            for _, row in df.iterrows():\n",
    "                combined.append({\n",
    "                    'emotion': emotion,\n",
    "                    'intensity': int(row['intensity']),  # ensure numeric\n",
    "                    'value': row[metric_name]\n",
    "                })\n",
    "        plot_df = pd.DataFrame(combined)\n",
    "        plot_df.sort_values(by='intensity', inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(\n",
    "            data=plot_df,\n",
    "            x='intensity',\n",
    "            y='value',\n",
    "            hue='emotion',\n",
    "            palette='Set2'\n",
    "        )\n",
    "        plt.title(f\"{model.upper()} – {metric_name.replace('_', ' ').title()} by Emotion and Intensity\")\n",
    "        plt.xlabel(\"Intensity (1–5)\")\n",
    "        plt.ylabel(metric_name.replace('_', ' ').title())\n",
    "        plt.xticks([0, 1, 2, 3, 4], ['1', '2', '3', '4', '5'])  # ensure correct labels\n",
    "        plt.legend(title=\"Emotion\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        filename = f\"{model}_{metric_name}_grouped.png\"\n",
    "        plt.savefig(filename)\n",
    "        print(f\"📊 Saved grouped plot → {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    # Load JSON files\n",
    "    model_files = {\n",
    "        'gemma': 'gemma_convos.json',\n",
    "        'llama': 'llama_convos.json'\n",
    "    }\n",
    "\n",
    "    # Initialize perplexity calculator\n",
    "    pp_calc = PerplexityCalculator('gpt2')\n",
    "\n",
    "    # Storage for all results\n",
    "    all_metrics = {}\n",
    "\n",
    "    for model_name, filepath in model_files.items():\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"❌ File not found: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)  # This is a dict of emotion → list of dicts\n",
    "\n",
    "        all_metrics[model_name] = {}\n",
    "\n",
    "        for emotion, responses in model_data.items():\n",
    "            print(f\"▶ Evaluating {model_name} / {emotion}...\")\n",
    "            df = evaluate_data(responses, pp_calc)\n",
    "            all_metrics[model_name][emotion] = df\n",
    "\n",
    "            out_file = f\"{model_name}_{emotion}_metrics.csv\"\n",
    "            df.to_csv(out_file, index=False)\n",
    "            print(f\"✔ Saved → {out_file}\")\n",
    "\n",
    "    # --- Plot comparison across models ---\n",
    "    for metric in ['readability', 'perplexity', 'lex_diversity']:\n",
    "        plot_grouped_metric(all_metrics, metric)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
