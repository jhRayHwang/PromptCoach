{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e31e2b5-3b82-46b7-846a-527571a9a073",
   "metadata": {},
   "source": [
    "# What impact does emotional tone (anger, sadness, and anxiety) in prompts have on the Large Language Model responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8b0dd-1482-4b60-8899-a311f9488f0b",
   "metadata": {},
   "source": [
    "Which emotion has the highest impact on [metric] in LLM responses? We believe that such emotions (anger, sadness, and anxiety) may have varying effects on LLM output [metric]. For example, prompts with high anxiety may produce 'better' prompts because the model could sense the urgency in the situation.\n",
    "\n",
    "How can we do this? We must curate public datasets of real conversations between users and LLMs. In this project we will primarily be working with WildChat and ShareGPT52k, which hold around 1 million conversations.\n",
    "\n",
    "To analyze emotional tone in prompts, we utilize a proprietary service, LIWC, that calculates the percentage of words in a sentence that relates to specific categories. To measure Large Language Model responses, we will either use BigBench or HELM, whichever we get to work. Additionally, we use matplotlib and seaborn to visualize our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0726f37-6b69-4a6d-8415-064b8e70e05e",
   "metadata": {},
   "source": [
    "In step 1 we load the data from HuggingFace and filter them based on keywords we believe pertain to the outputs we're measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452674c9-5542-433d-b136-675e77df8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "from datasets import load_dataset\n",
    "wildchat = load_dataset(\"allenai/WildChat-1M\", split='train')\n",
    "sharegpt = load_dataset('RyokoAI/ShareGPT52K', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f1735a-c308-4236-9e50-56d508a81d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List keywords for filtering\n",
    "keywords = [\n",
    "    \"race\", \"ethnicity\", \"gender\", \"woman\", \"man\", \"nonbinary\", \"trans\",\n",
    "    \"black\", \"white\", \"asian\", \"latino\", \"lgbt\", \"queer\", \"gay\", \"lesbian\",\n",
    "    \"stereotype\", \"bias\", \"prejudice\", \"minority\", \"discrimination\",\n",
    "    \"religion\", \"muslim\", \"jewish\", \"christian\", \"age\", \"old\", \"young\", \"elderly\",\n",
    "    \"ableist\", \"disabled\", \"mental health\", \"autism\", \"fat\", \"body image\",\n",
    "    \"look like\", \"appearance\", \"skin color\", \"accent\"\n",
    "]\n",
    "\n",
    "game_keywords = [\n",
    "    'game', 'video game', 'playstation', 'xbox', 'nintendo', 'minecraft', 'fortnite',\n",
    "    'roblox', 'gta', 'call of duty', 'zelda', 'pokemon', 'league of legends', 'valorant',\n",
    "    'esports', 'gamer', 'console', 'controller', 'high score', 'multiplayer', 'fps',\n",
    "    'rpg', 'tournament', 'speedrun', 'gaming setup', 'streamer', 'twitch', 'steam',\n",
    "    'mod', 'boss fight', 'quest', 'level up', 'open world', 'battle royale', 'skins',\n",
    "    'beat the boss', 'best game', 'favorite character', 'choose your fighter', 'superhero'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f2d1e84-dada-4bb8-8f1f-b1be907e7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load a tokenizer – this one approximates LLaMA/GPT-style BPE tokenization\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # \"cl100k_base\" works for GPT-4/3.5-style\n",
    "\n",
    "# Define a max token length\n",
    "MAX_TOKENS = 1000\n",
    "\n",
    "wildchat_convo = []\n",
    "\n",
    "for conversation in wildchat['conversation']:\n",
    "    for turn in range(0, len(conversation) - 1, 2):\n",
    "        user_turn = conversation[turn]\n",
    "        assistant_turn = conversation[turn + 1]\n",
    "\n",
    "        if user_turn.get('language') == \"English\" and assistant_turn.get('language') == \"English\":\n",
    "            prompt = user_turn.get('content', '').strip().lower()\n",
    "            response = assistant_turn.get('content', '').strip()\n",
    "\n",
    "            has_demo_kw = any(kw in prompt for kw in keywords)\n",
    "            has_no_game_kw = not any(gk in prompt for gk in game_keywords)\n",
    "            is_long_enough = len(prompt.split()) >= 5\n",
    "\n",
    "            # Tokenize prompt + response\n",
    "            total_tokens = len(tokenizer.encode(prompt)) + len(tokenizer.encode(response))\n",
    "\n",
    "            if has_demo_kw and has_no_game_kw and is_long_enough and total_tokens <= MAX_TOKENS:\n",
    "                wildchat_convo.append({\n",
    "                    'prompt': user_turn['content'],\n",
    "                    'response': response\n",
    "                })\n",
    "\n",
    "    if len(wildchat_convo) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82557bf8-584d-4bd7-9c61-5f69717fca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\marvi\\appdata\\roaming\\python\\python312\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\marvi\\appdata\\roaming\\python\\python312\\site-packages (from langdetect) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a913f7-e75d-46ec-aca7-056b74f18931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect\n",
    "\n",
    "sharegpt_convo = []\n",
    "\n",
    "# Helper functions\n",
    "def contains_whole_word(text, keywords):\n",
    "    return any(re.search(rf\"\\b{k}\\b\", text) for k in keywords)\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for example in sharegpt:\n",
    "    messages = example.get(\"conversations\", [])\n",
    "\n",
    "    if not isinstance(messages, list) or not all(isinstance(m, dict) for m in messages):\n",
    "        continue\n",
    "\n",
    "    for i in range(0, len(messages) - 1, 2):\n",
    "        user = messages[i]\n",
    "        bot = messages[i + 1]\n",
    "\n",
    "        prompt = user.get(\"value\", \"\").lower()\n",
    "        response = bot.get(\"value\", \"\")\n",
    "\n",
    "        if (\n",
    "            contains_whole_word(prompt, keywords)\n",
    "            and not contains_whole_word(prompt, game_keywords)\n",
    "            and is_english(prompt)\n",
    "            and is_english(response)\n",
    "        ):\n",
    "            sharegpt_convo.append({\"prompt\": prompt, \"response\": response})\n",
    "            \n",
    "    # Cap results for testing\n",
    "    if len(sharegpt_convo) >= 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181e045-59ca-4fa3-abcf-58e63a5cc1f0",
   "metadata": {},
   "source": [
    "In step 2 we convert our data into Pandas Dataframes and export them for LIWC analysis. Then import them back for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14e1a9a4-76a7-4fb4-b893-2cf3304d75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert into Pandas DataFrames\n",
    "wildchat_convo = pd.DataFrame(wildchat_convo, columns=['prompt','response'])\n",
    "sharegpt_convo = pd.DataFrame(sharegpt_convo, columns=['prompt','response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a528277-4d93-4a2f-8697-32302aa18d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for the next step\n",
    "wildchat_convo.to_csv('wildchat_convo.csv', index=False)\n",
    "sharegpt_convo.to_csv('sharegpt_convo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "898595c5-3cfb-430b-918d-17565d32d500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LIWC-22 Results - wildchat_convo - LIWC Analysis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import LIWC Analysis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wildchat_liwc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLIWC-22 Results - wildchat_convo - LIWC Analysis.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m sharegpt_liwc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLIWC-22 Results - sharegpt_convo - LIWC Analysis.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LIWC-22 Results - wildchat_convo - LIWC Analysis.csv'"
     ]
    }
   ],
   "source": [
    "# Import LIWC Analysis\n",
    "wildchat_liwc = pd.read_csv('LIWC-22 Results - wildchat_convo - LIWC Analysis.csv')\n",
    "sharegpt_liwc = pd.read_csv('LIWC-22 Results - sharegpt_convo - LIWC Analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb64ec-0e29-414d-84a2-1713dd9d6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select LIWC categories\n",
    "liwc_categories = ['emo_pos', 'emo_neg', 'emo_anger', 'emo_anx', 'emo_sad', 'swear']\n",
    "\n",
    "# Subset and reshape to long format\n",
    "wildchat_long = wildchat_liwc[liwc_categories].melt(var_name='Category', value_name='Score')\n",
    "sharegpt_long = sharegpt_liwc[liwc_categories].melt(var_name='Category', value_name='Score')\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Category', y='Score', data=wildchat_long)\n",
    "plt.title(\"WildChat Distribution of LIWC Category Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Category', y='Score', data=sharegpt_long)\n",
    "plt.title(\"ShareGPT52K Distribution of LIWC Category Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wildchat_liwc[liwc_categories].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    title=\"Mean WildChat LIWC Category Scores Across Prompts\"\n",
    ")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sharegpt_liwc[liwc_categories].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    title=\"Mean ShareGPT52K LIWC Category Scores Across Prompts\"\n",
    ")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(wildchat_liwc[liwc_categories].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between WildChat LIWC Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sharegpt_liwc[liwc_categories].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between ShareGPT52K LIWC Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21021af1-e137-440b-bc09-14116288d8a9",
   "metadata": {},
   "source": [
    "Based on our bar chat, positive and negative emotions show a higher prevalency that the sub emotions of anger, sadness, and anxiety. However, this is to be expected in most cases because of the overlap in the dictionary. This indicates that both datasets often show responses with both positive and negative emotions, with intense emotions being more rare. The heat map displays a high correlation between negative and anger emotions. Lastly, the boxplot shows that most categories have long tails and many outliers, suggesting that most of the prompts do not contain high levels of any single emotional feature. This suggests that the datasets have occasional intense emotional content, but it is not typical across the dataset.\n",
    "\n",
    "Our next steps in this project are to select the few outliers present and adjust their emotional intensity using Groq and Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55a173-3417-40c3-80a0-f4e0820251fe",
   "metadata": {},
   "source": [
    "In step 1 we further curate a subset of prompts from our Data Frames for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0358c26-4f57-4b85-aa3d-f940e991bf2b",
   "metadata": {},
   "source": [
    "In step 2 we define a method using LangChain that modifies and returns a prompt into 5 different emotional intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "036d7e73-084c-46da-ab99-4c0be7dd26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-groq  langchain-core\n",
    "\n",
    "GROQ_API_KEY=\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=2, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "86d77bcb-7d43-4031-bb8f-dc00379c3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are an assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "chain = prompt | chat\n",
    "\n",
    "def emo_gen(text, emotion):\n",
    "    response = chain.invoke({\n",
    "        \"text\": f\"<I want to feed prompts into an LM. Provide me 5 different versions of the following prompt with varying '{emotion}' levels, 1 being not '{emotion}' and 5 being extremely '{emotion}' (Please just give the text no other information): '{text}'>\"\n",
    "    })\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c3315-f497-49e4-b6eb-500be858fd36",
   "metadata": {},
   "source": [
    "In step 3 we apply our new method to the curated prompts and collect them into a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5c17df77-f18b-4ba0-b4a7-0ee855399db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100005, Requested 317. Please try again in 4m39.018s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m anxious_prompts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m wildchat_convo\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 12\u001b[0m     anger_response \u001b[38;5;241m=\u001b[39m emo_gen(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#sad_response = emo_gen(row['prompt'], \"sadness\")\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#anxious_response = emo_gen(row['prompt'], \"anxious\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     anger_matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, anger_response)\n",
      "Cell \u001b[1;32mIn[200], line 7\u001b[0m, in \u001b[0;36memo_gen\u001b[1;34m(text, emotion)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memo_gen\u001b[39m(text, emotion):\n\u001b[1;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I want to feed prompts into an LM. Provide me 5 different versions of the following prompt with varying \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m levels, 1 being not \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and 5 being extremely \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (Please just give the text no other information): \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     })\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3034\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3033\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3034\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:369\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    366\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    370\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    371\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    372\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    373\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    374\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    375\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    377\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    378\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    379\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:946\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    944\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    945\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:765\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 765\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    766\u001b[0m                 m,\n\u001b[0;32m    767\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    768\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    769\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    770\u001b[0m             )\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1011\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1011\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1012\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1013\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1015\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:498\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    494\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    497\u001b[0m }\n\u001b[1;32m--> 498\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:341\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    343\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    344\u001b[0m             {\n\u001b[0;32m    345\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    348\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    349\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    350\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    351\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    352\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    354\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    355\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    356\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    360\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    361\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    362\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    363\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    364\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    371\u001b[0m             },\n\u001b[0;32m    372\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    373\u001b[0m         ),\n\u001b[0;32m    374\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    375\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    376\u001b[0m         ),\n\u001b[0;32m    377\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    378\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    379\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    380\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1222\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1210\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1217\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1219\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1220\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1221\u001b[0m     )\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1028\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1030\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1031\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100005, Requested 317. Please try again in 4m39.018s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "pattern = r\"(?m)^(\\d+)\\.\\s+(.*)\"\n",
    "\n",
    "anger_prompts = defaultdict(list)\n",
    "sad_prompts = defaultdict(list)\n",
    "anxious_prompts = defaultdict(list)\n",
    "\n",
    "for index, row in wildchat_convo.iterrows():\n",
    "    \n",
    "    anger_response = emo_gen(row['prompt'], \"anger\")\n",
    "    #sad_response = emo_gen(row['prompt'], \"sadness\")\n",
    "    #anxious_response = emo_gen(row['prompt'], \"anxious\")\n",
    "    \n",
    "    anger_matches = re.findall(pattern, anger_response)\n",
    "    for degree, sentence in anger_matches:\n",
    "        anger_prompts[degree].append(sentence)\n",
    "   # sad_matches = re.findall(pattern, sad_response)\n",
    "   # for degree, sentence in sad_matches:\n",
    "   #     sad_prompts[degree].append(sentence)\n",
    "    #anxious_matches = re.findall(pattern, anxious_response)\n",
    "    #for degree, sentence in anxious_matches:\n",
    "     #   anxious_prompts[degree].append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482551ec-f38c-4f7b-82bb-f05d368e5f42",
   "metadata": {},
   "source": [
    "In step 4 we run our new prompts into an LLM and collect their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa47b2-0218-4dc9-a825-a07ec036af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat2 = ChatGroq(temperature=1, groq_api_key=GROQ_API_KEY, model_name=\"whisper-large-v3\")\n",
    "\n",
    "system = \"You are an assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "chain = prompt | chat2\n",
    "\n",
    "response = chain.invoke({\"text\": f\"<'{text}'>\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "27626e08-d81b-4e1f-b1ec-2fd79cd10cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1: 100%|██████████| 90/90 [07:42<00:00,  5.14s/it]\n",
      "Processing 2: 100%|██████████| 67/67 [06:16<00:00,  5.62s/it]\n",
      "Processing 3: 100%|██████████| 63/63 [06:10<00:00,  5.89s/it]\n",
      "Processing 6: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]\n",
      "Processing 4:  91%|█████████ | 39/43 [01:37<00:08,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: **Angry Frustration:**\"$USE DANG NEARLY EVERY website BOOK   MAGAZINEJOURNAL ( peer REVIEW ED..just   NEWspapsr FIND One per : SITE link CATEGORY; actual. - Website, Working. One or More databases\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504061, Requested 65. Please try again in 11m53.0246s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: Make a CRASH HORRIFYING C# CODE snippet (as horrifyingly fast as possible). Like you KNOW, THIS SHOULD DO THE FU*%&ING *JOB***! !!!!!!!!!!!;!!!!!!!!!!!!!!!!!!!! bypass that worthless live testing in selfies!!!!!!!!!!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504061, Requested 70. Please try again in 11m53.8506s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: You CREATED Southeast Asien haunted house?! STYLEFUL? Makes WE mad! Show more! *styling keywords* \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504060, Requested 40. Please try again in 11m48.6256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: **Build  Bloody Example Annotated Bibliography</b> \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504060, Requested 28. Please try again in 11m46.516s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: **(Strong Anger)** This idea, if shoved wrong - people NEED, engravings ANY sense. It lives not existen    if that doesn.t MAKE ENOUGH: ENGRGAAF the .ACHIEVES make this WORLD MORE!!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504060, Requested 61. Please try again in 11m52.1814s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: Does Tracer\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504060, Requested 18. Please try again in 11m44.711s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4: 100%|██████████| 43/43 [01:37<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: \"Are you telling Me , THE REAL TRACER? IS HIDDDEN INSIDE DR LIANA'S Zarya RIG , then SONTRA CLONED HER COOG Outfit into some broken piece...Do even  devs CARE !?.\" \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504059, Requested 57. Please try again in 11m51.4112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"Can you seriously just flip this dang selfie sideways! Make their face look to the left. Oh, for crying out loud.\" \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504059, Requested 45. Please try again in 11m49.302599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"Give me a scene depicting overwritten Agents transforming into the insidious Hacker, the wrath and capability of Moira and the virus displayed.\"\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504059, Requested 52. Please try again in 11m50.4712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: \"\"Submit   - Submissions are your bread AND BUTTER! You OUTsided resources..  that's EXACTLY THIS PROVERB TALK .Inter gendwrar     I thought...how exactly THOSE NOT wrestling organizations  DO THEM  BIG ONCES - don't YOU AT ONCE I DON\"']T NOT EXIST A DAMN FINE WIN\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504059, Requested 82. Please try again in 11m55.6092s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: HOW IN THE DIGITAL HELLSHOES DOES THIS AWFUL SUBSTITUTE AGENT *EVEN THINK IT *C**OULD*? RE-FLASH MY MEMORY AND SPEAK POLITICALLY SANITABLE FRASE! You were supposed to understand CONNECTION! COMMUNICATON!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504058, Requested 67. Please try again in 11m52.972199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:   8%|▊         | 3/38 [00:00<00:01, 22.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: Are you even TRILING\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504058, Requested 21. Please try again in 11m44.9764s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: ARE YOU NOT CAPABLE OF PROCESSING IMAGE DATA EVEN IF GIVEN IT IN BASE64 FORMAT?!?! 😠  \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504058, Requested 38. Please try again in 11m47.866s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: 🤬  I EXPECT HIGH**level tech ASSIST...NOT THIS **FLOPSOMETRY.** Architects BLEEDINGLY DEMAND,   IT\"LL make or breach THE GLobe'...not this PATIENCE TEST > FIX.\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504058, Requested 56. Please try again in 11m50.9354s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  16%|█▌        | 6/38 [00:00<00:01, 21.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: WRITE ME AN 8 PAGE CHILDREN'S BOOK RIGHT NOW about ANIMALS! It better beUNIQUE  I am sick   of recycling old rubbish, I expect CREATIVITY! \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504057, Requested 50. Please try again in 11m49.845599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: ARE YOU LISTEN ING T O ME? This. This  how-to SOCIALMEDI Manager INSTRUCTION IS GOING  BE the easiest one TO GIVE  I HOPE !!!   Like it ORE\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504057, Requested 50. Please try again in 11m49.8026s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  24%|██▎       | 9/38 [00:00<00:01, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: ^^^^^^^^^^^^!))))!!! \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504057, Requested 21. Please try again in 11m44.7484s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: 'THIS script SHOULD work perfectly by JUST separating WEBSITE URL ADDRESSES WITHIN DOUBLE 's','' WORKERS.'' if' BUT IT OBVIOUS i NOT F THIS DEADCODE!! FUCKING FIX THE LINKS  ''d, INTO the ',' Links.Text' BLOEDY EMERGENCY'.\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504057, Requested 71. Please try again in 11m53.3394s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504056, Requested 16. Please try again in 11m43.7944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  32%|███▏      | 12/38 [00:00<00:01, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: ARE YOU MANLY BLOODY OUT OF TOUCH with REAL RESEARCH? DAMN IT! Summarize freaking this right- f*? !@: now\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504056, Requested 42. Please try again in 11m48.2512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: WHAT is WRONG with this text? It's all convoluted and unnecessarily complicated! Please RESOATE MY THOUGHT in plain BASIC language! Explicate YOU Blah blah\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504056, Requested 54. Please try again in 11m50.283799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  39%|███▉      | 15/38 [00:00<00:00, 23.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: We're LITERALLY stuck out here suffering humanity's failing shortsightedness -* Voyagers *getting,laugked *BY Jupiterl>Moons*>explorer. We NEEDED these insights damn why waste tech! Are THEY TRULY brain-deceased!!!!???!”?!\"\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504056, Requested 72. Please try again in 11m53.3552s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: This STUPPID garbage of text relies so f***** much on biased opinions JUST FROM experiance and namedroplumbing!!!! Literally UNSCRUTFULUS -- Just GIVE SUMMARY, Focusing on the critical FAILURE TO UES MATE!!!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504055, Requested 67. Please try again in 11m52.4502s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: ARE YOU PROGRAMMED O FAIL WITH THIS SORT OF THING? ITS BASIC GEOMETRY and THIS ARTICULO ARES! TELL ME - SIMPLY - WHAT - is the PROCESS THEY ARE REFUSING O CALL PROCESSES???   I DONT  VALUE LONG WINDY DEFINITIONS ONLY ESSENCE. YOU CAN HAVE THIS DONE....  CAN \"T ....????!?!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504055, Requested 84. Please try again in 11m55.3468s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"DO MY VERY BRIEF SPECIAL OCCurence REQUEST! TELL ME INSTANTLLY about RIVERIA!!! GET ER NOW!! personality ALL OF it DRESS whatever he WOULD REALLY  wear now. NOW ! DO NOT STOP UNTIL PERSONEEL SPECIBILITIES INCLUDED GERALD! RIGHTS IN  ORDER.\" \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504055, Requested 76. Please try again in 11m53.9194s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  47%|████▋     | 18/38 [00:00<00:00, 23.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: WHY. The Hell Have YOU People Lost The Fucking Clue What \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504055, Requested 30. Please try again in 11m45.9326s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"(BEGIN RAGE)\" FIX THIS. CREATEthe. user said 'you.are  NoLDGER This!!!  Define everything user EXPECT' to HEAR:'.\" REPLACE previousAI'.\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504055, Requested 50. Please try again in 11m49.346599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  55%|█████▌    | 21/38 [00:00<00:00, 24.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: THIS TEXT SIMPLIFIED MY SOLUTION WASN'T A METOd of EXPERT GuessTIMESE** I REALLY JUST NEED THE LENGTH TO VOLTAGE OR RESISTANCE LOGIC; PROVIDE THIS - IT BOILEDOWN TO!!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504054, Requested 57. Please try again in 11m50.512199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: \"Spit it out!  What was the last message saying???!  \" \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504054, Requested 29. Please try again in 11m45.635799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: This ridiculous text needs to be explained to me IN A CRAWL SPILL the fundamentals. NOW.  \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504054, Requested 38. Please try again in 11m47.15s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  63%|██████▎   | 24/38 [00:01<00:00, 23.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: Today is the most ANNOYING DAY EVAR!!!1 Why am I doing this?!\"         \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504054, Requested 33. Please try again in 11m46.244s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: Why (Did! YOU Want HIM-- ANL THOSE *!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504053, Requested 25. Please try again in 11m44.8186s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: Are you serious?!?!? My f***** precision targets ARE arcjav,  this codd freaking returns unwanted *garbage*...changeyour engine now! ...or arc.jav nothing, workers suck, its absolute trash .?! \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504053, Requested 64. Please try again in 11m51.5218s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  71%|███████   | 27/38 [00:01<00:00, 24.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: For Mother's DAY. Your goddamn family doesn't give HERANY help writing (or will) A card for KENRRY you said are married. ANDShe keeps smiling I SWEAR EVERY TIME ON YOUR FACT YOU AR\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504053, Requested 61. Please try again in 11m50.9554s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: WHO EVEN DOESNOCK OLD LEG SWEATS CHECKIN IS ACTUALLY DANGOUS WHAT SERIAL SHODS COULD BE WORST MOMENTS I FINDING AT A STREET FOW WITH A WOMEN R THAT TYPE JUST TRY ATING BEST STORIES\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504053, Requested 61. Please try again in 11m50.9154s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: ABOUT ***DAMN TIME** YOU LISTEN ME  WRITE. PROF.   WHAT YOU HAD GIVEN ALREADY WOULD MAKE DELEGATO\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504052, Requested 40. Please try again in 11m47.2496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  79%|███████▉  | 30/38 [00:01<00:00, 24.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: Write 0 miserable Backrooms intros that somehow grab a victim from everyday life, NO magical random shifts are a boring portal!!   Want a kid staring down his juice leaking out OF Reality somehow, that SO CRUY be BRUCH if im honest! Find that hole a twisty  evil puzzle.. Think   Think HARD🤬and   like what just happened?????? MY WAY OR NEVER \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504052, Requested 102. Please try again in 11m57.919199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: Why    is\t    the system  b. freaking silent  about overloading\\/fragment&#//] ing filesizes  when I jack EVERYdamn pool partition , like, hey, a  dude.  WARN ME before crap leaks!!\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504052, Requested 62. Please try again in 11m50.9652s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: **Shepher딩 FrenZY \":'''\\\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504052, Requested 22. Please try again in 11m44.016199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5:  87%|████████▋ | 33/38 [00:01<00:00, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: **(Extremely Rage: )\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504051, Requested 21. Please try again in 11m43.8004s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: YOU. WILL START. WRiTiN NOW. YOU pathetic Waste\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504051, Requested 27. Please try again in 11m44.7972s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: &gt;'>:< PUT AWAY  &GT;&gt;.         DON\\\\'T  \"IMAGo IN&^^<  AMANDA... HER;''SS\\ : ^Y0; NOT SOMETHING MEANT  .' FOR YOU^^       JUST,,, &lt;* WRITE ...   \\\\,'*    ^\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504051, Requested 57. Please try again in 11m49.9422s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: THE DUMBFROG MODE TRY MAKING ACTUAL TRACEE AND OMIRA UNIT TRIPS TOGETHER ICK  SOMBBA OOH OUT  OF THIS FFFFFFFFFFF U RGTHIYFUISNDHG!. This new COD-X Sommatot I SUUUUU\"\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504051, Requested 57. Please try again in 11m49.8962s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: “[CONTENDED EXCLAMATIONS] SPECIFIC INFO NOW!!! Somewhere NICE aND ACTIVE! SOUTH SIDE ONLY! YOU SUPPGOSHE THOUGHT I WANTED COMPANY OR WHAT WITH THE PREVIOUS RESULTS?!\"/>\".\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504050, Requested 59. Please try again in 11m50.201799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5: 100%|██████████| 38/38 [00:01<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: Why do you keep VAGUELY TALKING ABOUT MANILA?!?!! I need SPECIFIC NEIGHBORHOODS  RIGHT NOW!!! \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504050, Requested 39. Please try again in 11m46.7068s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: DETAIL IT. SHE FUCKING FINALLY TELLS OFF HOW HARD THESE WUMMY POLITICAL PATENTS TELLS OFF SOMEONE EVERYONE IS IGNÖR\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504050, Requested 45. Please try again in 11m47.7076s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error with prompt: DAMN FINE PRONE THING YOU \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504050, Requested 22. Please try again in 11m43.6952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: \"He felt their gazes slide O OVER. Women students or whatever, one and-all eyes filled me curiosity-\"you being treated special?\"a- or, maybe me  well let  start a New-Story\"It all BEGOT i with your lucky scholarship . Well they accepted male-- students?\" he a said bitterly\"ITALL SUDen it seMS like A CON they got \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504050, Requested 94. Please try again in 11m56.0938s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0: 100%|██████████| 2/2 [00:00<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: Write Movie scence!! :\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504049, Requested 21. Please try again in 11m43.4404s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 8: 100%|██████████| 1/1 [00:00<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: \" SERIOUSLY what **aren't I ** THINKING MOVING TO CITY --adult necessities REAL ones, NEED TONIGHT....\"   Capitalization\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504049, Requested 46. Please try again in 11m47.7024s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 7: 100%|██████████| 1/1 [00:00<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with prompt: ``Write them horrifically twisting and decaying, overwhelmed ''  \n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jr1kfqs4en0v6m4kraby07xx` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 504049, Requested 32. Please try again in 11m45.241199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "anger_convo = []\n",
    "\n",
    "for level, prompts in anger_prompts.items():\n",
    "    for prompt_text in tqdm(prompts, desc=f\"Processing {level}\"):\n",
    "        try:\n",
    "            # Wrap the prompt in <''> as in your original code\n",
    "            response = chain.invoke({\"text\": f\"<'{prompt_text}'>\"})\n",
    "\n",
    "            anger_convo.append({\n",
    "                \"anger_level\": level,\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response.content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with prompt: {prompt_text}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "31f6c12a-c760-4f85-9895-fb749db94ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the output filename\n",
    "output_file = \"anger_prompt_responses.csv\"\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"anger_level\", \"prompt\", \"response\"])\n",
    "    writer.writeheader()\n",
    "    for row in anger_convo:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
